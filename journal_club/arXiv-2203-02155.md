# Training language models to follow instructions with human feedback

![image](https://github.com/user-attachments/assets/1a6eb471-b29c-46a4-9491-ba14c22382b0)

## InstructGPT: Method



![image](https://github.com/user-attachments/assets/aaeaf474-6182-47c6-ba79-8f19cacef348)

<p align="right"> Source: https://arxiv.org/pdf/2203.02155 </p>

### 1. Supervised fine-tuning (SFT)

- This step fine-tunes GPT-3 using demonstrations.
- Data: 13k training prompts


### 2. Reward model (RM) training

- Human labelers provide their preference ranking to a set of model models as a reward signal to train the reward model
- Data: 33k training prompts
- 
### 3. Proximal policy optimization (PPO)

- This step optimizes the policy with reward model using PPO
- Data: 31k training prompts
