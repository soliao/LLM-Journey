# Training language models to follow instructions with human feedback

![image](https://github.com/user-attachments/assets/1a6eb471-b29c-46a4-9491-ba14c22382b0)

## InstructGPT: Method



![image](https://github.com/user-attachments/assets/aaeaf474-6182-47c6-ba79-8f19cacef348)

<p align="right"> Source: https://arxiv.org/pdf/2203.02155 </p>

### 1. Supervised fine-tuning (SFT)

- This step fine-tunes GPT-3 using demonstrations.
- Data: 13k training prompts
- Model backbone: GPT-3
- Hyperparameters: 16 epochs; cosine learning rate decay; residual dropout 0.2
- Model selection: best RM score on the validation set
- Insights: despiting overfitting, training more epochs improves RM and human preference

### 2. Reward model (RM) training

- Human labelers provide their preference ranking to a set of model models as a reward signal to train the reward model
- Data: 33k training prompts
- Model: SFT model without the unembedding layer

### 3. Proximal policy optimization (PPO)

- This step optimizes the policy with reward model using PPO
- Data: 31k training prompts
